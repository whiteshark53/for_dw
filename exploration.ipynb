{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53bbac8b",
   "metadata": {},
   "source": [
    "Другое исследование на другом датасете (https://www.kaggle.com/datasets/hansamaldharmananda/infants-movements-kicking-patterns-data-set)\n",
    "\n",
    "Описание датасета:\n",
    "Содержит 7 различных типов движения младенцев для выявления неврологических расстройств,так как наша цель заключается в обнаружении ДЦП, это полезно.\n",
    "Расстройства:\n",
    "Ненормальная суетливость \n",
    "Хаотичный\n",
    "Синхронные судороги\n",
    "Скудный репертуар\n",
    "Обычная суетливость\n",
    "Обычное корчение\n",
    "Отсутствие движений\n",
    "Случайные (другие движения)\n",
    "\n",
    "5-мерный набор данных для каждого изображения RGB размером 50x50 (frame_width, frame_height, 3).\n",
    "\n",
    "1946 точек данных (выборок) с 40 кадрами (num_frames = размер окна) 1946x40x50x50x3 (выборки, num_frames, frame_width, frame_height, 3).\n",
    "1288 точек данных (выборок) с 60 кадрами (num_frames = размер окна) 1288x60x50x50x3 (выборки, num_frames, frame_width, frame_height, 3).\n",
    "767 точек данных (выборок) со 100 кадрами (num_frames = размер окна) 767x100x50x50x3 (выборки, num_frames, frame_width, frame_height, 3).Достаточно большой,чтобы не поймать переобучение,что часто бывает при недостаточном количестве данных.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c3e0e6",
   "metadata": {},
   "source": [
    "План работы\n",
    "\n",
    "1. Эффективная предварительная обработка:\n",
    "\n",
    "1.1 Уменьшение разрешения видео до 32x32 пикселей\n",
    "\n",
    "1.2 Обрезка видео до 20 кадров (с дополнением нулями при необходимости)(напрямую связано с проблемой краша ядра из-за переполнения памяти, упираемся в мощности)\n",
    "\n",
    "1.3 Нормализация пикселей в диапазон [0, 1]\n",
    "\n",
    "2. Генератор данных:\n",
    "\n",
    "2.1 Постепенная загрузка и обработка данных\n",
    "\n",
    "2.2 Пакетная обработка для экономии памяти\n",
    "\n",
    "2.3 Автоматическое перемешивание данных\n",
    "\n",
    "3. Компактная 3D CNN модель (выбрано под мощности ПК):\n",
    "\n",
    "3.1 Оптимизированная архитектура для работы на CPU\n",
    "\n",
    "3.2 Global Average Pooling вместо полносвязных слоев\n",
    "\n",
    "3.3 Batch Normalization для стабилизации обучения\n",
    "\n",
    "3.4 Dropout для регуляризации\n",
    "\n",
    "4. Управление памятью:\n",
    "\n",
    "4.1 Явное освобождение памяти после обработки файлов\n",
    "\n",
    "4.2 Сборка мусора между этапами обработки\n",
    "\n",
    "4.3 Пакетное прогнозирование для оценки модели\n",
    "\n",
    "5. Оценка модели:\n",
    "\n",
    "5.1 Подробный отчет классификации\n",
    "\n",
    "5.2 Матрица ошибок\n",
    "\n",
    "5.3 Графики обучения (accuracy, loss, precision, recall)\n",
    "\n",
    "5.4 Расчет времени выполнения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4ffdb3",
   "metadata": {},
   "source": [
    "Результаты:\n",
    "\n",
    "Модель сохраняется как infant_movement_classifier.h5\n",
    "\n",
    "Отчет классификации - classification_report.txt\n",
    "\n",
    "Графики обучения - training_metrics.png\n",
    "\n",
    "Подробный лог выполнения - infant_movement_classification.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f638aa3",
   "metadata": {},
   "source": [
    "Как говорилось ранее достаточно много данных следовательно:\n",
    "\n",
    "Оптимизации для работы с большими данными:\n",
    "1. Уменьшение размерности:\n",
    "\n",
    "Видео сокращено до 20 кадров\n",
    "\n",
    "Разрешение уменьшено до 32x32 пикселей\n",
    "\n",
    "Использование float32 вместо float64\n",
    "\n",
    "2. Эффективная обработка памяти:\n",
    "\n",
    "Поэтапная загрузка файлов\n",
    "\n",
    "Очистка памяти после обработки каждого файла\n",
    "\n",
    "Пакетная обработка данных\n",
    "\n",
    "3. Адаптированная модель:\n",
    "\n",
    "Минимальное количество параметров\n",
    "\n",
    "Глобальный пулинг вместо полносвязных слоев\n",
    "\n",
    "Оптимальный размер батча\n",
    "\n",
    "4. Контроль обучения:\n",
    "\n",
    "Early Stopping для предотвращения переобучения\n",
    "\n",
    "ReduceLROnPlateau для адаптивной настройки learning rate\n",
    "\n",
    "Мониторинг нескольких метрик"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c7a828",
   "metadata": {},
   "source": [
    "Так как я загрузила тетрадку на коллаб (для удобства просмотра результатов запуска), то скопирую полученные результаты вместе с расшифровкой сюда, для просмотра всех файлов можно перейти на гитхаб (ссылка) или я могу прислать архив."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfbbc5d",
   "metadata": {},
   "source": [
    "Результаты метрик:\n",
    "\n",
    "                           precision     recall    f1-score    support\n",
    "\n",
    "        Abnormal fidgety       0.97      0.85      0.90        98\n",
    "                 Chaotic       1.00      1.00      1.00        64\n",
    "      Cramp synchronized       1.00      0.87      0.93        85\n",
    "         Poor repertoire       1.00      1.00      1.00       112\n",
    "          Normal fidgety       0.81      1.00      0.89        95\n",
    "         Normal writhing       1.00      1.00      1.00       101\n",
    "            No movements       1.00      1.00      1.00       104\n",
    "Random (Other movements)       1.00      1.00      1.00       142\n",
    "\n",
    "                accuracy                           0.97       801\n",
    "               macro avg       0.97      0.96      0.97       801\n",
    "            weighted avg       0.97      0.97      0.97       801\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568eed22",
   "metadata": {},
   "source": [
    "Основные выводы:\n",
    "\n",
    "Высокая общая эффективность модели:\n",
    "\n",
    "Общая точность (accuracy) составляет 97% - модель демонстрирует отличную способность различать все 8 классов движений\n",
    "\n",
    "Средние метрики (macro avg и weighted avg) находятся на уровне 0.97 по всем показателям\n",
    "\n",
    "Идеальное распознавание большинства классов:\n",
    "\n",
    "5 из 8 классов имеют идеальные показатели (1.00) по precision, recall и f1-score:\n",
    "\n",
    "Chaotic\n",
    "\n",
    "Poor repertoire\n",
    "\n",
    "Normal writhing\n",
    "\n",
    "No movements\n",
    "\n",
    "Random (Other movements)\n",
    "\n",
    "Это свидетельствует, что модель безошибочно идентифицирует эти движения\n",
    "\n",
    "Ключевые области для улучшения:\n",
    "\n",
    "Abnormal fidgety:\n",
    "\n",
    "Precision 0.97 (высокий) - когда модель предсказывает этот класс, она почти всегда права\n",
    "\n",
    "Recall 0.85 (ниже) - модель пропускает около 15% реальных случаев аномальных движений\n",
    "\n",
    "Надо: Увеличить выборку аномальных движений или применить аугментацию данных\n",
    "\n",
    "Cramp synchronized:\n",
    "\n",
    "Precision 1.00 (идеальный) - все предсказания этого класса верны\n",
    "\n",
    "Recall 0.87 - модель не обнаруживает 13% реальных случаев\n",
    "\n",
    "Надо: Исследовать особенности пропущенных случаев\n",
    "\n",
    "Normal fidgety:\n",
    "\n",
    "Precision 0.81 (относительно низкий) - 19% предсказаний этого класса ошибочны\n",
    "\n",
    "Recall 1.00 - модель находит все реальные случаи\n",
    "\n",
    "Надо: Уточнить различия между нормальными и аномальными движениями\n",
    "\n",
    "Клинически значимые выводы:\n",
    "Надежность обнаружения критических состояний:\n",
    "\n",
    "Модель идеально определяет опасные движения типа \"Chaotic\" и \"Cramp synchronized\"\n",
    "\n",
    "Высокая точность обнаружения \"Abnormal fidgety\" (97%) позволяет надежно выявлять неврологические нарушения\n",
    "\n",
    "Эффективность скрининга:\n",
    "\n",
    "Идеальное распознавание отсутствия движений (\"No movements\") снижает риск ложных тревог\n",
    "\n",
    "Высокие показатели для \"Normal writhing\" гарантируют корректную идентификацию здоровых младенцев\n",
    "\n",
    "Для повышения надежности диагностики аномальных движений (особенно recall для Abnormal fidgety) рекомендуется:\n",
    "\n",
    "Увеличить выборку редких движений\n",
    "\n",
    "Внедрить механизмы attention в модель\n",
    "\n",
    "Провести дополнительную проверку случаев с низкой уверенностью предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c5642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy in /home/whiteshark53/miniconda3/envs/torch_env/lib/python3.8/site-packages (1.24.4)\n",
      "Requirement already satisfied: scikit-learn in /home/whiteshark53/.local/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: matplotlib in /home/whiteshark53/miniconda3/envs/torch_env/lib/python3.8/site-packages (3.7.5)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/whiteshark53/.local/lib/python3.8/site-packages (from tensorflow) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /home/whiteshark53/.local/lib/python3.8/site-packages (from tensorflow) (24.3.25)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/whiteshark53/.local/lib/python3.8/site-packages (from tensorflow) (1.67.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/whiteshark53/.local/lib/python3.8/site-packages (from tensorflow) (3.11.0)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow)\n",
      "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /home/whiteshark53/miniconda3/envs/torch_env/lib/python3.8/site-packages (from tensorflow) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in /home/whiteshark53/miniconda3/envs/torch_env/lib/python3.8/site-packages (from tensorflow) (75.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/whiteshark53/miniconda3/envs/torch_env/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow)\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow)\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow)\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/whiteshark53/miniconda3/envs/torch_env/lib/python3.8/site-packages (from tensorflow) (1.17.2)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/whiteshark53/.local/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/whiteshark53/.local/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/whiteshark53/.local/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/whiteshark53/.local/lib/python3.8/site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/whiteshark53/.local/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/whiteshark53/.local/lib/python3.8/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/whiteshark53/.local/lib/python3.8/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/whiteshark53/.local/lib/python3.8/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/whiteshark53/.local/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/whiteshark53/.local/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/whiteshark53/.local/lib/python3.8/site-packages (from matplotlib) (6.1.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/whiteshark53/.local/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/whiteshark53/.local/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/whiteshark53/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/whiteshark53/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/whiteshark53/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/whiteshark53/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/whiteshark53/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/whiteshark53/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/whiteshark53/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.5.0)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/whiteshark53/.local/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/whiteshark53/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (4.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/whiteshark53/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/whiteshark53/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/whiteshark53/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/whiteshark53/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/whiteshark53/.local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Downloading tensorflow-2.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m479.6/479.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:06\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "Downloading tensorflow_io_gcs_filesystem-0.34.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: libclang, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, pyasn1, protobuf, opt-einsum, oauthlib, numpy, keras, google-pasta, gast, astunparse, rsa, pyasn1-modules, tensorboard, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.28.2\n",
      "    Uninstalling protobuf-5.28.2:\n",
      "      Successfully uninstalled protobuf-5.28.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.14.0\n",
      "    Uninstalling tensorboard-2.14.0:\n",
      "      Successfully uninstalled tensorboard-2.14.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "altair 5.4.1 requires jsonschema>=3.0, which is not installed.\n",
      "clearml 1.15.1 requires jsonschema>=2.6.0, which is not installed.\n",
      "altair 5.4.1 requires typing-extensions>=4.10.0; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
      "fastapi 0.115.0 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydantic 2.8.2 requires typing-extensions>=4.6.1; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydantic-core 2.20.1 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "exceptiongroup 1.3.0 requires typing-extensions>=4.6.0; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
      "openai 1.88.0 requires typing-extensions<5,>=4.11, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "torch 2.4.1+cpu requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed astunparse-1.6.3 gast-0.4.0 google-pasta-0.2.0 keras-2.13.1 libclang-18.1.1 numpy-1.24.3 oauthlib-3.3.1 opt-einsum-3.4.0 protobuf-4.25.8 pyasn1-0.6.1 pyasn1-modules-0.4.2 rsa-4.9.1 tensorboard-2.13.0 tensorflow-2.13.1 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.4.0 typing-extensions-4.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow numpy scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab3112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-01 06:55:03,451 - INFO - ==================================================\n",
      "2025-07-01 06:55:03,452 - INFO - ЗАПУСК КЛАССИФИКАЦИИ ДВИЖЕНИЙ МЛАДЕНЦЕВ\n",
      "2025-07-01 06:55:03,452 - INFO - ==================================================\n",
      "2025-07-01 06:55:03,453 - INFO - Загрузка ./data/data_40_50_50.npz и ./data/target_40_50_50.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-01 06:55:06,250 - INFO - Обработано 100/1946 видео\n",
      "2025-07-01 06:55:06,297 - INFO - Обработано 200/1946 видео\n",
      "2025-07-01 06:55:06,348 - INFO - Обработано 300/1946 видео\n",
      "2025-07-01 06:55:06,415 - INFO - Обработано 400/1946 видео\n",
      "2025-07-01 06:55:06,475 - INFO - Обработано 500/1946 видео\n",
      "2025-07-01 06:55:06,535 - INFO - Обработано 600/1946 видео\n",
      "2025-07-01 06:55:06,596 - INFO - Обработано 700/1946 видео\n",
      "2025-07-01 06:55:06,665 - INFO - Обработано 800/1946 видео\n",
      "2025-07-01 06:55:06,738 - INFO - Обработано 900/1946 видео\n",
      "2025-07-01 06:55:06,808 - INFO - Обработано 1000/1946 видео\n",
      "2025-07-01 06:55:06,878 - INFO - Обработано 1100/1946 видео\n",
      "2025-07-01 06:55:06,951 - INFO - Обработано 1200/1946 видео\n",
      "2025-07-01 06:55:07,019 - INFO - Обработано 1300/1946 видео\n",
      "2025-07-01 06:55:07,092 - INFO - Обработано 1400/1946 видео\n",
      "2025-07-01 06:55:07,132 - INFO - Обработано 1500/1946 видео\n",
      "2025-07-01 06:55:07,172 - INFO - Обработано 1600/1946 видео\n",
      "2025-07-01 06:55:07,213 - INFO - Обработано 1700/1946 видео\n",
      "2025-07-01 06:55:07,254 - INFO - Обработано 1800/1946 видео\n",
      "2025-07-01 06:55:07,296 - INFO - Обработано 1900/1946 видео\n",
      "2025-07-01 06:55:07,490 - INFO - Обработано: data_40_50_50.npz - 1946 samples\n",
      "2025-07-01 06:55:07,639 - INFO - Загрузка ./data/data_60_50_50.npz и ./data/target_60_50_50.npz\n",
      "2025-07-01 06:55:10,229 - INFO - Обработано 100/1288 видео\n",
      "2025-07-01 06:55:10,265 - INFO - Обработано 200/1288 видео\n",
      "2025-07-01 06:55:10,304 - INFO - Обработано 300/1288 видео\n",
      "2025-07-01 06:55:10,336 - INFO - Обработано 400/1288 видео\n",
      "2025-07-01 06:55:10,366 - INFO - Обработано 500/1288 видео\n",
      "2025-07-01 06:55:10,398 - INFO - Обработано 600/1288 видео\n",
      "2025-07-01 06:55:10,428 - INFO - Обработано 700/1288 видео\n",
      "2025-07-01 06:55:10,459 - INFO - Обработано 800/1288 видео\n",
      "2025-07-01 06:55:10,490 - INFO - Обработано 900/1288 видео\n",
      "2025-07-01 06:55:10,531 - INFO - Обработано 1000/1288 видео\n",
      "2025-07-01 06:55:10,563 - INFO - Обработано 1100/1288 видео\n",
      "2025-07-01 06:55:10,597 - INFO - Обработано 1200/1288 видео\n",
      "2025-07-01 06:55:10,727 - INFO - Обработано: data_60_50_50.npz - 1288 samples\n",
      "2025-07-01 06:55:10,839 - INFO - Загрузка ./data/data_100_50_50.npz и ./data/target_100_50_50.npz\n",
      "2025-07-01 06:55:13,336 - INFO - Обработано 100/767 видео\n",
      "2025-07-01 06:55:13,370 - INFO - Обработано 200/767 видео\n",
      "2025-07-01 06:55:13,403 - INFO - Обработано 300/767 видео\n",
      "2025-07-01 06:55:13,439 - INFO - Обработано 400/767 видео\n",
      "2025-07-01 06:55:13,469 - INFO - Обработано 500/767 видео\n",
      "2025-07-01 06:55:13,498 - INFO - Обработано 600/767 видео\n",
      "2025-07-01 06:55:13,528 - INFO - Обработано 700/767 видео\n",
      "2025-07-01 06:55:13,576 - INFO - Обработано: data_100_50_50.npz - 767 samples\n",
      "2025-07-01 06:55:13,879 - INFO - Общая форма данных: (4001, 20, 32, 32, 3)\n",
      "2025-07-01 06:55:13,880 - INFO - Распределение классов: [491 319 426 561 476 503 517 708]\n",
      "2025-07-01 06:55:14,419 - INFO - Model: \"sequential_1\"\n",
      "2025-07-01 06:55:14,420 - INFO - _________________________________________________________________\n",
      "2025-07-01 06:55:14,420 - INFO -  Layer (type)                Output Shape              Param #   \n",
      "2025-07-01 06:55:14,421 - INFO - =================================================================\n",
      "2025-07-01 06:55:14,422 - INFO -  conv3d_2 (Conv3D)           (None, 20, 32, 32, 8)     656       \n",
      "2025-07-01 06:55:14,423 - INFO -                                                                  \n",
      "2025-07-01 06:55:14,423 - INFO -  batch_normalization_2 (Bat  (None, 20, 32, 32, 8)     32        \n",
      "2025-07-01 06:55:14,424 - INFO -  chNormalization)                                                \n",
      "2025-07-01 06:55:14,425 - INFO -                                                                  \n",
      "2025-07-01 06:55:14,426 - INFO -  max_pooling3d_2 (MaxPoolin  (None, 20, 16, 16, 8)     0         \n",
      "2025-07-01 06:55:14,427 - INFO -  g3D)                                                            \n",
      "2025-07-01 06:55:14,428 - INFO -                                                                  \n",
      "2025-07-01 06:55:14,429 - INFO -  dropout_3 (Dropout)         (None, 20, 16, 16, 8)     0         \n",
      "2025-07-01 06:55:14,429 - INFO -                                                                  \n",
      "2025-07-01 06:55:14,430 - INFO -  conv3d_3 (Conv3D)           (None, 20, 16, 16, 16)    3472      \n",
      "2025-07-01 06:55:14,431 - INFO -                                                                  \n",
      "2025-07-01 06:55:14,433 - INFO -  batch_normalization_3 (Bat  (None, 20, 16, 16, 16)    64        \n",
      "2025-07-01 06:55:14,434 - INFO -  chNormalization)                                                \n",
      "2025-07-01 06:55:14,434 - INFO -                                                                  \n",
      "2025-07-01 06:55:14,435 - INFO -  max_pooling3d_3 (MaxPoolin  (None, 10, 8, 8, 16)      0         \n",
      "2025-07-01 06:55:14,436 - INFO -  g3D)                                                            \n",
      "2025-07-01 06:55:14,437 - INFO -                                                                  \n",
      "2025-07-01 06:55:14,438 - INFO -  dropout_4 (Dropout)         (None, 10, 8, 8, 16)      0         \n",
      "2025-07-01 06:55:14,439 - INFO -                                                                  \n",
      "2025-07-01 06:55:14,440 - INFO -  global_average_pooling3d_1  (None, 16)                0         \n",
      "2025-07-01 06:55:14,441 - INFO -   (GlobalAveragePooling3D)                                       \n",
      "2025-07-01 06:55:14,442 - INFO -                                                                  \n",
      "2025-07-01 06:55:14,443 - INFO -  dense_2 (Dense)             (None, 32)                544       \n",
      "2025-07-01 06:55:14,444 - INFO -                                                                  \n",
      "2025-07-01 06:55:14,445 - INFO -  dropout_5 (Dropout)         (None, 32)                0         \n",
      "2025-07-01 06:55:14,446 - INFO -                                                                  \n",
      "2025-07-01 06:55:14,448 - INFO -  dense_3 (Dense)             (None, 8)                 264       \n",
      "2025-07-01 06:55:14,448 - INFO -                                                                  \n",
      "2025-07-01 06:55:14,450 - INFO - =================================================================\n",
      "2025-07-01 06:55:14,451 - INFO - Total params: 5032 (19.66 KB)\n",
      "2025-07-01 06:55:14,452 - INFO - Trainable params: 4984 (19.47 KB)\n",
      "2025-07-01 06:55:14,453 - INFO - Non-trainable params: 48 (192.00 Byte)\n",
      "2025-07-01 06:55:14,453 - INFO - _________________________________________________________________\n",
      "2025-07-01 06:55:14,454 - INFO - Начало обучения...\n",
      "Epoch 1/15\n",
      "800/800 [==============================] - 78s 96ms/step - loss: 1.8803 - accuracy: 0.2797 - precision: 0.7112 - recall: 0.0516 - auc: 0.6824 - val_loss: 2.1106 - val_accuracy: 0.1800 - val_precision: 0.4830 - val_recall: 0.1775 - val_auc: 0.5946 - lr: 5.0000e-04\n",
      "Epoch 2/15\n",
      "800/800 [==============================] - 74s 93ms/step - loss: 1.6486 - accuracy: 0.4000 - precision: 0.8756 - recall: 0.1144 - auc: 0.7858 - val_loss: 1.7104 - val_accuracy: 0.2585 - val_precision: 1.0000 - val_recall: 0.2246 - val_auc: 0.7127 - lr: 5.0000e-04\n",
      "Epoch 3/15\n",
      "800/800 [==============================] - 75s 93ms/step - loss: 1.3701 - accuracy: 0.5044 - precision: 0.9469 - recall: 0.2175 - auc: 0.8666 - val_loss: 1.0751 - val_accuracy: 0.7215 - val_precision: 1.0000 - val_recall: 0.3551 - val_auc: 0.9519 - lr: 5.0000e-04\n",
      "Epoch 4/15\n",
      "800/800 [==============================] - 74s 93ms/step - loss: 1.1201 - accuracy: 0.5891 - precision: 0.9254 - recall: 0.3372 - auc: 0.9163 - val_loss: 0.8391 - val_accuracy: 0.7553 - val_precision: 1.0000 - val_recall: 0.3701 - val_auc: 0.9747 - lr: 5.0000e-04\n",
      "Epoch 5/15\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.9549 - accuracy: 0.6653 - precision: 0.9170 - recall: 0.4141 - auc: 0.9435 - val_loss: 1.1710 - val_accuracy: 0.5797 - val_precision: 0.9099 - val_recall: 0.2535 - val_auc: 0.8968 - lr: 5.0000e-04\n",
      "Epoch 6/15\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.8095 - accuracy: 0.7212 - precision: 0.9256 - recall: 0.4897 - auc: 0.9622 - val_loss: 0.6657 - val_accuracy: 0.7892 - val_precision: 0.9942 - val_recall: 0.6462 - val_auc: 0.9799 - lr: 5.0000e-04\n",
      "Epoch 7/15\n",
      "800/800 [==============================] - 75s 93ms/step - loss: 0.7075 - accuracy: 0.7613 - precision: 0.9321 - recall: 0.5619 - auc: 0.9724 - val_loss: 0.8857 - val_accuracy: 0.5809 - val_precision: 1.0000 - val_recall: 0.5696 - val_auc: 0.9394 - lr: 5.0000e-04\n",
      "Epoch 8/15\n",
      "800/800 [==============================] - 74s 93ms/step - loss: 0.6141 - accuracy: 0.7956 - precision: 0.9269 - recall: 0.6341 - auc: 0.9798 - val_loss: 3.0022 - val_accuracy: 0.1669 - val_precision: 0.2285 - val_recall: 0.1669 - val_auc: 0.6882 - lr: 5.0000e-04\n",
      "Epoch 9/15\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.5392 - accuracy: 0.8263 - precision: 0.9269 - recall: 0.6894 - auc: 0.9844 - val_loss: 0.3248 - val_accuracy: 0.9561 - val_precision: 1.0000 - val_recall: 0.8143 - val_auc: 0.9995 - lr: 5.0000e-04\n",
      "Epoch 10/15\n",
      "800/800 [==============================] - 76s 94ms/step - loss: 0.4644 - accuracy: 0.8619 - precision: 0.9387 - recall: 0.7509 - auc: 0.9893 - val_loss: 0.2828 - val_accuracy: 0.9636 - val_precision: 1.0000 - val_recall: 0.8808 - val_auc: 0.9994 - lr: 5.0000e-04\n",
      "Epoch 11/15\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.4115 - accuracy: 0.8772 - precision: 0.9442 - recall: 0.7928 - auc: 0.9914 - val_loss: 0.7165 - val_accuracy: 0.7102 - val_precision: 0.8412 - val_recall: 0.6512 - val_auc: 0.9623 - lr: 5.0000e-04\n",
      "Epoch 12/15\n",
      "800/800 [==============================] - 77s 96ms/step - loss: 0.3588 - accuracy: 0.8866 - precision: 0.9416 - recall: 0.8213 - auc: 0.9936 - val_loss: 0.3792 - val_accuracy: 0.8670 - val_precision: 1.0000 - val_recall: 0.7478 - val_auc: 0.9938 - lr: 5.0000e-04\n",
      "Epoch 13/15\n",
      "800/800 [==============================] - 76s 95ms/step - loss: 0.3124 - accuracy: 0.9137 - precision: 0.9500 - recall: 0.8609 - auc: 0.9952 - val_loss: 0.1466 - val_accuracy: 0.9950 - val_precision: 1.0000 - val_recall: 0.9724 - val_auc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 14/15\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.2977 - accuracy: 0.9106 - precision: 0.9464 - recall: 0.8600 - auc: 0.9954 - val_loss: 0.9105 - val_accuracy: 0.6324 - val_precision: 0.7376 - val_recall: 0.5960 - val_auc: 0.9472 - lr: 5.0000e-04\n",
      "Epoch 15/15\n",
      "800/800 [==============================] - 75s 94ms/step - loss: 0.2790 - accuracy: 0.9162 - precision: 0.9486 - recall: 0.8769 - auc: 0.9960 - val_loss: 0.1491 - val_accuracy: 0.9674 - val_precision: 0.9781 - val_recall: 0.9536 - val_auc: 0.9997 - lr: 5.0000e-04\n",
      "2025-07-01 07:14:03,322 - INFO - Модель сохранена как 'infant_movement_classifier.h5'\n",
      "2025-07-01 07:14:03,323 - INFO - Оценка модели...\n",
      "2025-07-01 07:14:03,431 - INFO - Обработано 4/801 валидационных образцов\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whiteshark53/miniconda3/envs/torch_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-01 07:14:04,010 - INFO - Обработано 44/801 валидационных образцов\n",
      "2025-07-01 07:14:04,587 - INFO - Обработано 84/801 валидационных образцов\n",
      "2025-07-01 07:14:05,170 - INFO - Обработано 124/801 валидационных образцов\n",
      "2025-07-01 07:14:05,745 - INFO - Обработано 164/801 валидационных образцов\n",
      "2025-07-01 07:14:06,329 - INFO - Обработано 204/801 валидационных образцов\n",
      "2025-07-01 07:14:06,895 - INFO - Обработано 244/801 валидационных образцов\n",
      "2025-07-01 07:14:07,471 - INFO - Обработано 284/801 валидационных образцов\n",
      "2025-07-01 07:14:08,043 - INFO - Обработано 324/801 валидационных образцов\n",
      "2025-07-01 07:14:08,611 - INFO - Обработано 364/801 валидационных образцов\n",
      "2025-07-01 07:14:09,183 - INFO - Обработано 404/801 валидационных образцов\n",
      "2025-07-01 07:14:09,755 - INFO - Обработано 444/801 валидационных образцов\n",
      "2025-07-01 07:14:10,331 - INFO - Обработано 484/801 валидационных образцов\n",
      "2025-07-01 07:14:10,898 - INFO - Обработано 524/801 валидационных образцов\n",
      "2025-07-01 07:14:11,474 - INFO - Обработано 564/801 валидационных образцов\n",
      "2025-07-01 07:14:12,040 - INFO - Обработано 604/801 валидационных образцов\n",
      "2025-07-01 07:14:12,615 - INFO - Обработано 644/801 валидационных образцов\n",
      "2025-07-01 07:14:13,193 - INFO - Обработано 684/801 валидационных образцов\n",
      "2025-07-01 07:14:13,773 - INFO - Обработано 724/801 валидационных образцов\n",
      "2025-07-01 07:14:14,341 - INFO - Обработано 764/801 валидационных образцов\n",
      "2025-07-01 07:14:14,894 - INFO - Обработано 804/801 валидационных образцов\n",
      "2025-07-01 07:14:14,895 - INFO - \n",
      "Classification Report:\n",
      "2025-07-01 07:14:14,901 - INFO -                           precision    recall  f1-score   support\n",
      "\n",
      "        Abnormal fidgety       0.97      0.85      0.90        98\n",
      "                 Chaotic       1.00      1.00      1.00        64\n",
      "      Cramp synchronized       1.00      0.87      0.93        85\n",
      "         Poor repertoire       1.00      1.00      1.00       112\n",
      "          Normal fidgety       0.81      1.00      0.89        95\n",
      "         Normal writhing       1.00      1.00      1.00       101\n",
      "            No movements       1.00      1.00      1.00       104\n",
      "Random (Other movements)       1.00      1.00      1.00       142\n",
      "\n",
      "                accuracy                           0.97       801\n",
      "               macro avg       0.97      0.96      0.97       801\n",
      "            weighted avg       0.97      0.97      0.97       801\n",
      "\n",
      "2025-07-01 07:14:14,903 - INFO - \n",
      "Confusion Matrix:\n",
      "2025-07-01 07:14:14,903 - INFO - [[ 83   0   0   0  15   0   0   0]\n",
      " [  0  64   0   0   0   0   0   0]\n",
      " [  3   0  74   0   8   0   0   0]\n",
      " [  0   0   0 112   0   0   0   0]\n",
      " [  0   0   0   0  95   0   0   0]\n",
      " [  0   0   0   0   0 101   0   0]\n",
      " [  0   0   0   0   0   0 104   0]\n",
      " [  0   0   0   0   0   0   0 142]]\n",
      "2025-07-01 07:14:15,357 - INFO - Общее время выполнения: 19.20 минут\n",
      "2025-07-01 07:14:15,358 - INFO - ==================================================\n",
      "2025-07-01 07:14:15,359 - INFO - ПРОГРАММА ЗАВЕРШЕНА\n",
      "2025-07-01 07:14:15,360 - INFO - ==================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Conv3D, MaxPooling3D, BatchNormalization, \n",
    "                                     GlobalAveragePooling3D, Dense, Dropout)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import logging\n",
    "import gc\n",
    "import time\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"infant_movement_classification.log\"),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger('infant_movement')\n",
    "\n",
    "MAX_FRAMES = 20\n",
    "NEW_SIZE = 32\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 15\n",
    "NUM_CLASSES = 8\n",
    "DATA_PATH = './data'\n",
    "\n",
    "CLASS_MAPPING = {\n",
    "    \"Abnormal fidgety\": 0,\n",
    "    \"Chaotic\": 1,\n",
    "    \"Cramp synchronized\": 2,\n",
    "    \"Poor repertoire\": 3,\n",
    "    \"Normal fidgety\": 4,\n",
    "    \"Normal writhing\": 5,\n",
    "    \"No movements\": 6,\n",
    "    \"Random (Other movements)\": 7\n",
    "}\n",
    "\n",
    "def reduce_video_resolution(video, new_size=NEW_SIZE):\n",
    "    \"\"\"Уменьшение разрешения видео с помощью билинейной интерполяции\"\"\"\n",
    "    return tf.image.resize(video, [new_size, new_size])\n",
    "\n",
    "def preprocess_video(video, max_frames=MAX_FRAMES, new_size=NEW_SIZE):\n",
    "    \"\"\"Предварительная обработка одного видео\"\"\"\n",
    "    if video.shape[0] > max_frames:\n",
    "        video = video[:max_frames]\n",
    "    else:\n",
    "        pad = np.zeros((max_frames - video.shape[0], *video.shape[1:]), dtype=video.dtype)\n",
    "        video = np.concatenate([video, pad], axis=0)\n",
    "    \n",
    "    video_resized = reduce_video_resolution(video, new_size)\n",
    "    return video_resized.numpy()\n",
    "\n",
    "def load_and_process_data(file_path, target_path):\n",
    "    \"\"\"Загрузка и обработка данных из файлов\"\"\"\n",
    "    logger.info(f\"Загрузка {file_path} и {target_path}\")\n",
    "    \n",
    "    with np.load(file_path) as data:\n",
    "        videos = data['arr_0']\n",
    "    with np.load(target_path) as target:\n",
    "        labels = target['arr_0']\n",
    "    \n",
    "    processed_videos = []\n",
    "    for i, video in enumerate(videos):\n",
    "        processed_videos.append(preprocess_video(video))\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            logger.info(f\"Обработано {i+1}/{len(videos)} видео\")\n",
    "    \n",
    "    return np.array(processed_videos), labels\n",
    "\n",
    "def create_data_generator(videos, labels, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    \"\"\"Создание генератора данных\"\"\"\n",
    "    num_samples = len(videos)\n",
    "    indices = np.arange(num_samples)\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    while True:\n",
    "        for start_idx in range(0, num_samples, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, num_samples)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            batch_videos = videos[batch_indices]\n",
    "            batch_labels = labels[batch_indices]\n",
    "            \n",
    "            batch_videos = batch_videos.astype('float32') / 255.0\n",
    "            batch_labels = to_categorical(batch_labels, num_classes=NUM_CLASSES)\n",
    "            \n",
    "            yield batch_videos, batch_labels\n",
    "\n",
    "def create_compact_3d_cnn(input_shape, num_classes):\n",
    "    \"\"\"Создание компактной 3D CNN модели\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv3D(8, (3, 3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling3D((1, 2, 2)),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Conv3D(16, (3, 3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling3D((2, 2, 2)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        GlobalAveragePooling3D(),\n",
    "        \n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def plot_metrics(history):\n",
    "    \"\"\"Построение графиков метрик обучения\"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history.history['precision'], label='Train Precision')\n",
    "    plt.plot(history.history['val_precision'], label='Validation Precision')\n",
    "    plt.title('Training and Validation Precision')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(history.history['recall'], label='Train Recall')\n",
    "    plt.plot(history.history['val_recall'], label='Validation Recall')\n",
    "    plt.title('Training and Validation Recall')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_metrics.png')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        logger.error(f\"Директория с данными не найдена: {DATA_PATH}\")\n",
    "        return\n",
    "    \n",
    "    all_videos = []\n",
    "    all_labels = []\n",
    "    \n",
    "    file_pairs = [\n",
    "        ('data_40_50_50.npz', 'target_40_50_50.npz'),\n",
    "        ('data_60_50_50.npz', 'target_60_50_50.npz'),\n",
    "        ('data_100_50_50.npz', 'target_100_50_50.npz')\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for data_file, target_file in file_pairs:\n",
    "        data_path = os.path.join(DATA_PATH, data_file)\n",
    "        target_path = os.path.join(DATA_PATH, target_file)\n",
    "        \n",
    "        if not os.path.exists(data_path) or not os.path.exists(target_path):\n",
    "            logger.warning(f\"Файлы не найдены: {data_file} или {target_file}\")\n",
    "            continue\n",
    "        \n",
    "        videos, labels = load_and_process_data(data_path, target_path)\n",
    "        all_videos.append(videos)\n",
    "        all_labels.append(labels)\n",
    "        \n",
    "        logger.info(f\"Обработано: {data_file} - {videos.shape[0]} samples\")\n",
    "        gc.collect()\n",
    "    \n",
    "    if not all_videos:\n",
    "        logger.error(\"Не удалось загрузить данные\")\n",
    "        return\n",
    "    \n",
    "    X = np.concatenate(all_videos)\n",
    "    y = np.concatenate(all_labels)\n",
    "    \n",
    "    logger.info(f\"Общая форма данных: {X.shape}\")\n",
    "    logger.info(f\"Распределение классов: {np.bincount(y)}\")\n",
    "    \n",
    "    del all_videos, all_labels\n",
    "    gc.collect()\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    del X, y\n",
    "    gc.collect()\n",
    "    \n",
    "    train_gen = create_data_generator(X_train, y_train, BATCH_SIZE)\n",
    "    val_gen = create_data_generator(X_val, y_val, BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    train_steps = len(X_train) // BATCH_SIZE\n",
    "    val_steps = len(X_val) // BATCH_SIZE\n",
    "    \n",
    "    input_shape = (MAX_FRAMES, NEW_SIZE, NEW_SIZE, 3)\n",
    "    model = create_compact_3d_cnn(input_shape, NUM_CLASSES)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0005),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='auc')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    model.summary(print_fn=lambda x: logger.info(x))\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "    ]\n",
    "    \n",
    "    logger.info(\"Начало обучения...\")\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_data=val_gen,\n",
    "        validation_steps=val_steps,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model.save('infant_movement_classifier.h5')\n",
    "    logger.info(\"Модель сохранена как 'infant_movement_classifier.h5'\")\n",
    "    \n",
    "    logger.info(\"Оценка модели...\")\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for i in range(0, len(X_val), BATCH_SIZE):\n",
    "        batch_X = X_val[i:i+BATCH_SIZE].astype('float32') / 255.0\n",
    "        batch_y = y_val[i:i+BATCH_SIZE]\n",
    "        \n",
    "        preds = model.predict(batch_X, verbose=0)\n",
    "        y_pred.extend(np.argmax(preds, axis=1))\n",
    "        y_true.extend(batch_y)\n",
    "        \n",
    "        if (i // BATCH_SIZE) % 10 == 0:\n",
    "            logger.info(f\"Обработано {i+BATCH_SIZE}/{len(X_val)} валидационных образцов\")\n",
    "    \n",
    "    logger.info(\"\\nClassification Report:\")\n",
    "    cr = classification_report(y_true, y_pred, target_names=list(CLASS_MAPPING.keys()))\n",
    "    logger.info(cr)\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    logger.info(\"\\nConfusion Matrix:\")\n",
    "    logger.info(cm)\n",
    "    \n",
    "    with open(\"classification_report.txt\", \"w\") as f:\n",
    "        f.write(cr)\n",
    "    \n",
    "    plot_metrics(history)\n",
    "    \n",
    "    logger.info(f\"Общее время выполнения: {(time.time()-start_time)/60:.2f} минут\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(\"ЗАПУСК КЛАССИФИКАЦИИ ДВИЖЕНИЙ МЛАДЕНЦЕВ\")\n",
    "    logger.info(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Критическая ошибка:\")\n",
    "    \n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(\"ПРОГРАММА ЗАВЕРШЕНА\")\n",
    "    logger.info(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch 1.8.0",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
